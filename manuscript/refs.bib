@article{narasimhan2020,
	title = {{deconvolveR}: {A} {G}-{Modeling} {Program} for {Deconvolution} and {Empirical} {Bayes} {Estimation}},
	volume = {94},
	copyright = {Copyright (c) 2020 Balasubramanian Narasimhan, Bradley Efron},
	issn = {1548-7660},
	shorttitle = {{deconvolveR}},
	url = {https://doi.org/10.18637/jss.v094.i11},
	doi = {10.18637/jss.v094.i11},
	abstract = {Empirical Bayes inference assumes an unknown prior density g(θ) has yielded (unobservables) Θ1, Θ2, ..., ΘN, and each Θi produces an independent observation Xi from pi (Xi {\textbar} Θi). The marginal density fi (Xi) is a convolution of the prior g and pi. The Bayes deconvolution problem is one of recovering g from the data. Although estimation of g - so called g-modeling - is difficult, the results are more encouraging if the prior g is restricted to lie within a parametric family of distributions. We present a deconvolution approach where g is restricted to be in a parametric exponential family, along with an R package deconvolveR designed for the purpose.},
	language = {en},
	urldate = {2023-02-15},
	journal = {Journal of Statistical Software},
	author = {Narasimhan, Balasubramanian and Efron, Bradley},
	month = sep,
	year = {2020},
	keywords = {Bayes deconvolution, empirical Bayes, g-modeling, missing species, R package deconvolveR},
	pages = {1--20},
	file = {Full Text:/Users/johannes/Zotero/storage/J2RM9JAC/Narasimhan and Efron - 2020 - deconvolveR A G-Modeling Program for Deconvolutio.pdf:application/pdf},
}

@article{efron2016a,
	title = {Empirical {Bayes} deconvolution estimates},
	volume = {103},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asv068},
	doi = {10.1093/biomet/asv068},
	abstract = {An unknown prior density g(θ ) has yielded realizations 1, . . . , N . They are unobservable, but each i produces an observable value Xi according to a known probability mechanism, such as Xi ∼ Po( i ). We wish to estimate g(θ ) from the observed sample X1, . . . , X N . Traditional asymptotic calculations are discouraging, indicating very slow nonparametric rates of convergence. In this article we show that parametric exponential family modelling of g(θ ) can give useful estimates in moderate-sized samples. We illustrate the approach with a variety of real and artificial examples. Covariate information can be incorporated into the deconvolution process, leading to a more detailed theory of generalized linear mixed models.},
	language = {en},
	number = {1},
	urldate = {2023-02-15},
	journal = {Biometrika},
	author = {Efron, Bradley},
	month = mar,
	year = {2016},
	pages = {1--20},
	file = {Efron - 2016 - Empirical Bayes deconvolution estimates.pdf:/Users/johannes/Zotero/storage/U5GA2FGF/Efron - 2016 - Empirical Bayes deconvolution estimates.pdf:application/pdf;Empirical Bayes deconvolution estimates:/Users/johannes/Zotero/storage/ZLJXSHGT/efron2016.pdf.pdf:application/pdf},
}

@article{vanzwet2021,
	title = {The statistical properties of {RCTs} and a proposal for shrinkage},
	volume = {40},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9173},
	doi = {10.1002/sim.9173},
	abstract = {We abstract the concept of a randomized controlled trial as a triple (β,b,s), where β is the primary efficacy parameter, b the estimate, and s the standard error (s{\textgreater}0). If the parameter β is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z=b/s and the signal-to-noise ratio SNR=β/s from a sample of pairs (bi,si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on (β,b,s) only through the pair (z,SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13\%. We also consider the exaggeration ratio which is the factor by which the magnitude of β is overestimated. We find that if the estimate is just significant at the 5\% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.},
	language = {en},
	number = {27},
	urldate = {2023-02-15},
	journal = {Statistics in Medicine},
	author = {van Zwet, Erik and Schwab, Simon and Senn, Stephen},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9173},
	keywords = {achieved power, Cochrane review, exaggeration, randomized controlled trial, type M error},
	pages = {6107--6117},
	file = {Full Text PDF:/Users/johannes/Zotero/storage/2AKGGVAS/van Zwet et al. - 2021 - The statistical properties of RCTs and a proposal .pdf:application/pdf;Snapshot:/Users/johannes/Zotero/storage/ZWQ4QYA7/sim.html:text/html;The statistical properties of RCTs and a proposal for shrinkage:/Users/johannes/Zotero/storage/PE6KZEL8/zwet2021.pdf.pdf:application/pdf},
}

@misc{vanzwet2020,
	title = {The {Significance} {Filter}, the {Winner}'s {Curse} and the {Need} to {Shrink}},
	url = {http://arxiv.org/abs/2009.09440},
	doi = {10.48550/arXiv.2009.09440},
	abstract = {The "significance filter" refers to focusing exclusively on statistically significant results. Since frequentist properties such as unbiasedness and coverage are valid only before the data have been observed, there are no guarantees if we condition on significance. In fact, the significance filter leads to overestimation of the magnitude of the parameter, which has been called the "winner's curse". It can also lead to undercoverage of the confidence interval. Moreover, these problems become more severe if the power is low. While these issues clearly deserve our attention, they have been studied only informally and mathematical results are lacking. Here we study them from the frequentist and the Bayesian perspective. We prove that the relative bias of the magnitude is a decreasing function of the power and that the usual confidence interval undercovers when the power is less than 50\%. We conclude that failure to apply the appropriate amount of shrinkage can lead to misleading inferences.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {van Zwet, Erik and Cator, Eric},
	month = sep,
	year = {2020},
	note = {arXiv:2009.09440 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/XMXIDRCY/van Zwet and Cator - 2020 - The Significance Filter, the Winner's Curse and th.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/KBDG5HK7/2009.html:text/html},
}

@misc{vanzwet2020a,
	title = {A proposal for informative default priors scaled by the standard error of estimates},
	url = {http://arxiv.org/abs/2011.15037},
	doi = {10.48550/arXiv.2011.15037},
	abstract = {If we have an unbiased estimate of some parameter of interest, then its absolute value is positively biased for the absolute value of the parameter. This bias is large when the signal-to-noise ratio (SNR) is small, and it becomes even larger when we condition on statistical significance; the winner's curse. This is a frequentist motivation for regularization. To determine a suitable amount of shrinkage, we propose to estimate the distribution of the SNR from a large collection or corpus of similar studies and use this as a prior distribution. The wider the scope of the corpus, the less informative the prior, but a wider scope does not necessarily result in a more diffuse prior. We show that the estimation of the prior simplifies if we require that posterior inference is equivariant under linear transformations of the data. We demonstrate our approach with corpora of 86 replication studies from psychology and 178 phase 3 clinical trials. Our suggestion is not intended to be a replacement for a prior based on full information about a particular problem; rather, it represents a familywise choice that should yield better long-term properties than the current default uniform prior, which has led to systematic overestimates of effect sizes and a replication crisis when these inflated estimates have not shown up in later studies.},
	urldate = {2023-02-15},
	publisher = {arXiv},
	author = {van Zwet, Erik and Gelman, Andrew},
	month = nov,
	year = {2020},
	note = {arXiv:2011.15037 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 20 pages, 4 figures},
	file = {arXiv Fulltext PDF:/Users/johannes/Zotero/storage/33IV7C6N/van Zwet and Gelman - 2020 - A proposal for informative default priors scaled b.pdf:application/pdf;arXiv.org Snapshot:/Users/johannes/Zotero/storage/JGKIY3ZK/2011.html:text/html},
}

@book{efron2010,
	address = {Cambridge ; New York},
	series = {Institute of mathematical statistics monographs},
	title = {Large-scale inference: empirical {Bayes} methods for estimation, testing, and prediction},
	isbn = {978-0-521-19249-1},
	shorttitle = {Large-scale inference},
	language = {en},
	number = {1},
	publisher = {Cambridge University Press},
	author = {Efron, Bradley},
	year = {2010},
	note = {OCLC: ocn639166324},
	keywords = {Bayesian statistical decision theory},
	file = {Efron - 2010 - Large-scale inference empirical Bayes methods for.pdf:/Users/johannes/Zotero/storage/6QSIAIJC/Efron - 2010 - Large-scale inference empirical Bayes methods for.pdf:application/pdf},
}

@article{carroll1988,
	title = {Optimal {Rates} of {Convergence} for {Deconvolving} a {Density}},
	volume = {83},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478718},
	doi = {10.1080/01621459.1988.10478718},
	abstract = {Suppose that the sum of two independent random variables X and Z is observed, where Z denotes measurement error and has a known distribution, and where the unknown density f of X is to be estimated. One application is the estimation of a prior density for a sequence of location parameters. A second application arises in the errors-in-variables problem for nonlinear and generalized linear models, when one attempts to model the distribution of the true but unobservable covariates. This article shows that if Z is normally distributed and f has k bounded derivatives, then the fastest attainable convergence rate of any nonparametric estimator of f is only (log n)–k/2. Therefore, deconvolution with normal errors may not be a practical proposition. Other error distributions are also treated. Stefanski—Carroll (1987a) estimators achieve the optimal rates. The results given have versions for multiplicative errors, where they imply that even optimal rates are exceptionally slow.},
	number = {404},
	urldate = {2023-02-19},
	journal = {Journal of the American Statistical Association},
	author = {Carroll, Raymond J. and Hall, Peter},
	month = dec,
	year = {1988},
	note = {Publisher: Taylor \& Francis
  eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478718},
	keywords = {Measurement error, Deconvolution, Density estimation, Errors in variables, Rates of convergence},
	pages = {1184--1186},
	file = {Optimal Rates of Convergence for Deconvolving a Density:/Users/johannes/Zotero/storage/XJ8ZAWM6/carroll1988.pdf.pdf:application/pdf},
}


@article{efron2014,
	title = {Two {Modeling} {Strategies} for {Empirical} {Bayes} {Estimation}},
	volume = {29},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-29/issue-2/Two-Modeling-Strategies-for-Empirical-Bayes-Estimation/10.1214/13-STS455.full},
	doi = {10.1214/13-STS455},
	abstract = {Empirical Bayes methods use the data from parallel experiments, for instance, observations \$X\_\{k\}{\textbackslash}sim{\textbackslash}mathcal\{N\}({\textbackslash}Theta\_\{k\},1)\$ for \$k=1,2,{\textbackslash}ldots,N\$, to estimate the conditional distributions \${\textbackslash}Theta\_\{k\}{\textbar}X\_\{k\}\$. There are two main estimation strategies: modeling on the \${\textbackslash}theta\$ space, called “\$g\$-modeling” here, and modeling on the \$x\$ space, called “\$f\$-modeling.” The two approaches are described and compared. A series of computational formulas are developed to assess their frequentist accuracy. Several examples, both contrived and genuine, show the strengths and limitations of the two strategies.},
	number = {2},
	urldate = {2023-02-23},
	journal = {Statistical Science},
	author = {Efron, Bradley},
	month = may,
	year = {2014},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {\$f\$-modeling, \$g\$-modeling, Bayes rule in terms of \$f\$, prior exponential families},
	pages = {285--301},
	file = {Full Text PDF:/Users/johannes/Zotero/storage/DHB57KKX/Efron - 2014 - Two Modeling Strategies for Empirical Bayes Estima.pdf:application/pdf;Two Modeling Strategies for Empirical Bayes Estimation:/Users/johannes/Zotero/storage/6IMMQ8I8/efron2014.pdf.pdf:application/pdf},
}

@article{casella1985,
	title = {An {Introduction} to {Empirical} {Bayes} {Data} {Analysis}},
	volume = {39},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2682801},
	doi = {10.2307/2682801},
	abstract = {Empirical Bayes methods have been shown to be powerful data-analysis tools in recent years. The empirical Bayes model is much richer than either the classical or the ordinary Bayes model and often provides superior estimates of parameters. An introduction to some empirical Bayes methods is given, and these methods are illustrated with two examples.},
	number = {2},
	urldate = {2023-02-25},
	journal = {The American Statistician},
	author = {Casella, George},
	year = {1985},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {83--87},
	file = {An Introduction to Empirical Bayes Data Analysis:/Users/johannes/Zotero/storage/XZDTJ77E/casella1985.pdf.pdf:application/pdf;Submitted Version:/Users/johannes/Zotero/storage/J9BLKIMY/Casella - 1985 - An Introduction to Empirical Bayes Data Analysis.pdf:application/pdf},
}


@book{efron2016c,
	address = {New York, NY},
	series = {Institute of {Mathematical} {Statistics} monographs},
	title = {Computer age statistical inference: algorithms, evidence, and data science},
	isbn = {978-1-107-14989-2},
	shorttitle = {Computer age statistical inference},
	publisher = {Cambridge University Press},
	author = {Efron, Bradley and Hastie, Trevor},
	year = {2016},
	keywords = {Mathematical statistics, Data processing},
}


@article{higgins2009,
	title = {A re-evaluation of random-effects meta-analysis},
	volume = {172},
	issn = {09641998, 1467985X},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2008.00552.x},
	doi = {10.1111/j.1467-985X.2008.00552.x},
	abstract = {Meta-analysis in the presence of unexplained heterogeneity is frequently undertaken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justiﬁcation and interpretation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without problems, including computational intensity and sensitivity to a priori judgements. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of ‘set shifting’ ability in people with eating disorders.},
	language = {en},
	number = {1},
	urldate = {2023-02-13},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Higgins, Julian P. T. and Thompson, Simon G. and Spiegelhalter, David J.},
	month = jan,
	year = {2009},
	pages = {137--159},
	file = {Higgins et al. - 2009 - A re-evaluation of random-effects meta-analysis.pdf:/Users/johannes/Zotero/storage/N5UJIVE4/Higgins et al. - 2009 - A re-evaluation of random-effects meta-analysis.pdf:application/pdf},
}


@article{chung2013,
	title = {Avoiding zero between-study variance estimates in random-effects meta-analysis},
	volume = {32},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5821},
	doi = {10.1002/sim.5821},
	abstract = {Fixed-effects meta-analysis has been criticized because the assumption of homogeneity is often unrealistic and can result in underestimation of parameter uncertainty. Random-effects meta-analysis and meta-regression are therefore typically used to accommodate explained and unexplained between-study variability. However, it is not unusual to obtain a boundary estimate of zero for the (residual) between-study standard deviation, resulting in fixed-effects estimates of the other parameters and their standard errors. To avoid such boundary estimates, we suggest using Bayes modal (BM) estimation with a gamma prior on the between-study standard deviation. When no prior information is available regarding the magnitude of the between-study standard deviation, a weakly informative default prior can be used (with shape parameter 2 and rate parameter close to 0) that produces positive estimates but does not overrule the data, leading to only a small decrease in the log likelihood from its maximum. We review the most commonly used estimation methods for meta-analysis and meta-regression including classical and Bayesian methods and apply these methods, as well as our BM estimator, to real datasets. We then perform simulations to compare BM estimation with the other methods and find that BM estimation performs well by (i) avoiding boundary estimates; (ii) having smaller root mean squared error for the between-study standard deviation; and (iii) better coverage for the overall effects than the other methods when the true model has at least a small or moderate amount of unexplained heterogeneity. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {23},
	urldate = {2023-03-08},
	journal = {Statistics in Medicine},
	author = {Chung, Yeojin and Rabe-Hesketh, Sophia and Choi, In-Hee},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5821},
	keywords = {Bayesian posterior mode, penalized maximum likelihood, random-effects meta-analysis, variance estimation},
	pages = {4071--4089},
	file = {Full Text PDF:/Users/johannes/Zotero/storage/BYX3A5VI/Chung et al. - 2013 - Avoiding zero between-study variance estimates in .pdf:application/pdf;Snapshot:/Users/johannes/Zotero/storage/CVICEIEI/sim.html:text/html},
}


@article{viechtbauer2005,
	title = {Bias and {Efficiency} of {Meta}-{Analytic} {Variance} {Estimators} in the {Random}-{Effects} {Model}},
	volume = {30},
	issn = {1076-9986, 1935-1054},
	url = {http://journals.sagepub.com/doi/10.3102/10769986030003261},
	doi = {10.3102/10769986030003261},
	abstract = {The meta-analytic random effects model assumes that the variability in effect size estimates drawn from a set of studies can be decomposed into two parts: heterogeneity due to random population effects and sampling variance. In this context, the usual goal is to estimate the central tendency and the amount of heterogeneity in the population effect sizes. The amount of heterogeneity in a set of effect sizes has implications regarding the interpretation of the meta-analytic ﬁndings and often serves as an indicator for the presence of potential moderator variables. Five population heterogeneity estimators were compared in this article analytically and via Monte Carlo simulations with respect to their bias and efﬁciency.},
	language = {en},
	number = {3},
	urldate = {2023-03-11},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Viechtbauer, Wolfgang},
	month = sep,
	year = {2005},
	pages = {261--293},
	file = {Viechtbauer - 2005 - Bias and Efficiency of Meta-Analytic Variance Esti.pdf:/Users/johannes/Zotero/storage/TE22VKV3/Viechtbauer - 2005 - Bias and Efficiency of Meta-Analytic Variance Esti.pdf:application/pdf},
}

  
@article{harville1977,
	title = {Maximum {Likelihood} {Approaches} to {Variance} {Component} {Estimation} and to {Related} {Problems}},
	volume = {72},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2286796},
	doi = {10.2307/2286796},
	abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ML) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (REML) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ML estimators of variance components. There are many iterative algorithms that can be considered for computing the ML or REML estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
	number = {358},
	urldate = {2023-03-27},
	journal = {Journal of the American Statistical Association},
	author = {Harville, David A.},
	year = {1977},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {320--338},
}


@book{lehmann1998,
	address = {New York},
	edition = {2nd ed},
	series = {Springer texts in statistics},
	title = {Theory of point estimation},
	isbn = {978-0-387-98502-2},
	publisher = {Springer},
	author = {Lehmann, E. L. and Casella, George},
	year = {1998},
	keywords = {Fix-point estimation},
}

@article{mogensen2018,
  author  = {Mogensen, Patrick Kofod and Riseth, Asbj{\o}rn Nilsen},
  title   = {Optim: A mathematical optimization package for {Julia}},
  journal = {Journal of Open Source Software},
  year    = {2018},
  volume  = {3},
  number  = {24},
  pages   = {615},
  doi     = {10.21105/joss.00615}
}

@article{brockwell2001,
	title = {A comparison of statistical methods for meta-analysis},
	volume = {20},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.650},
	doi = {10.1002/sim.650},
	abstract = {Meta-analysis may be used to estimate an overall effect across a number of similar studies. A number of statistical techniques are currently used to combine individual study results. The simplest of these is based on a fixed effects model, which assumes the true effect is the same for all studies. A random effects model, however, allows the true effect to vary across studies, with the mean true effect the parameter of interest. We consider three methods currently used for estimation within the framework of a random effects model, and illustrate them by applying each method to a collection of six studies on the effect of aspirin after myocardial infarction. These methods are compared using estimated coverage probabilities of confidence intervals for the overall effect. The techniques considered all generally have coverages below the nominal level, and in particular it is shown that the commonly used DerSimonian and Laird method does not adequately reflect the error associated with parameter estimation, especially when the number of studies is small. Copyright © 2001 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2023-04-14},
	journal = {Statistics in Medicine},
	author = {Brockwell, Sarah E. and Gordon, Ian R.},
	year = {2001},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.650},
	pages = {825--840},
	file = {Full Text PDF:/Users/johannes/Zotero/storage/FD2JRZ4U/Brockwell and Gordon - 2001 - A comparison of statistical methods for meta-analy.pdf:application/pdf;Snapshot:/Users/johannes/Zotero/storage/RBW6JKWM/sim.html:text/html},
}


@article{raudenbush1985,
 ISSN = {03629791},
 URL = {http://www.jstor.org/stable/1164836},
 abstract = {As interest in quantitative research synthesis grows, investigators increasingly seek to use information about study features-study contexts, designs, treatments, and subjects-to account for variation in study outcomes. To facilitate analysis of diverse study findings, a mixed linear model with fixed and random effects is presented and illustrated with data from teacher expectancy experiments. This strategy enables the analyst to (a) estimate the variance of the effect size parameters by means of maximum likelihood; (b) pose a series of linear models to explain the effect parameter variance; (c) use information about study characteristics to derive improved empirical Bayes estimates of individual study effect sizes; and (d) examine the sensitivity of all substantive inferences to likely errors in the estimation of variance components.},
 author = {Stephen W. Raudenbush and Anthony S. Bryk},
 journal = {Journal of Educational Statistics},
 number = {2},
 pages = {75--98},
 publisher = {[Sage Publications, Inc., American Educational Research Association, American Statistical Association]},
 title = {Empirical Bayes Meta-Analysis},
 urldate = {2023-04-15},
 volume = {10},
 year = {1985}
}

@article{Julia2017,
    title={Julia: A fresh approach to numerical computing},
    author={Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B},
    journal={SIAM {R}eview},
    volume={59},
    number={1},
    pages={65--98},
    year={2017},
    publisher={SIAM},
    doi={10.1137/141000671},
    url={https://epubs.siam.org/doi/10.1137/141000671}
}

@article{distributionsjl,
   author = {Mathieu Besançon and Theodore Papamarkou and David Anthoff and Alex Arslan and Simon Byrne and Dahua Lin and John Pearson},
   title = {Distributions.jl: Definition and Modeling of Probability Distributions in the JuliaStats Ecosystem},
   journal = {Journal of Statistical Software},
   volume = {98},
   number = {16},
   year = {2021},
   keywords = {Julia; distributions; modeling; interface; mixture; KDE; sampling; probabilistic programming; inference},
   issn = {1548-7660},
   pages = {1--30},
   doi = {10.18637/jss.v098.i16},
   url = {https://www.jstatsoft.org/v098/i16}
}
